{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf8a3c62-216c-446d-851d-7249cc67787a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {align:left;display:block} \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {align:left;display:block} \n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e27c8c-7f53-4e9f-b4da-7e0343fb6313",
   "metadata": {},
   "source": [
    "# Markov Decision Process (MDP)\n",
    "----\n",
    "\n",
    "**Value Iteration Process with Policy Changes in MDP**\n",
    "\n",
    "We begin with a Markov Decision Process (MDP) where an agent decides whether to invest conservatively (C) or aggressively (A) in a financial portfolio. The objective is to find an optimal policy maximizing long-term rewards.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Defining the MDP Components**\n",
    "\n",
    "**States (S):**\n",
    "\n",
    "- Low Wealth (L)\n",
    "- Medium Wealth (M)\n",
    "- High Wealth (H)\n",
    "\n",
    "**Actions (A):**\n",
    "\n",
    "- Conservative (C)\n",
    "- Aggressive (A)\n",
    "\n",
    "**Transition Probabilities:**\n",
    "\n",
    "| Current State | Action | Next State Probabilities     |\n",
    "| ------------- | ------ | ---------------------------- |\n",
    "| Low (L)       | C      | 80% Stay in L, 20% Move to M |\n",
    "| Low (L)       | A      | 60% Stay in L, 40% Move to M |\n",
    "| Medium (M)    | C      | 70% Stay in M, 30% Move to H |\n",
    "| Medium (M)    | A      | 50% Stay in M, 50% Move to H |\n",
    "| High (H)      | C      | 90% Stay in H, 10% Drop to M |\n",
    "| High (H)      | A      | 70% Stay in H, 30% Drop to M |\n",
    "\n",
    "**Rewards:**\n",
    "\n",
    "- Low Wealth (L): -1\n",
    "- Medium Wealth (M): 3\n",
    "- High Wealth (H): 5\n",
    "\n",
    "**Discount Factor (γ):** 0.9\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf6becd-538b-4010-b4ed-c2e03c6c2e1a",
   "metadata": {},
   "source": [
    "### **Step 2: Value Iteration Updates**\n",
    "\n",
    "We initialize values: $V_0(L) = 0$, $V_0(M) = 0$, $V_0(H) = 0$.\n",
    "\n",
    "#### **Iteration 1**\n",
    "\n",
    "Using Bellman’s equation:\n",
    "\n",
    ">$\n",
    "V_1(s) = \\max_a \\left[ R(s) + \\gamma \\sum_{s'} P(s' | s, a) V_0(s') \\right]\n",
    "$\n",
    "\n",
    "For **Low Wealth (L):**\n",
    "\n",
    ">$\n",
    "V_1(L) = \\max \\left[ -1 + 0.9(0.8V_0(L) + 0.2V_0(M)), -1 + 0.9(0.6V_0(L) + 0.4V_0(M)) \\right]\n",
    "$\n",
    "\n",
    "For **Medium Wealth (M):**\n",
    "\n",
    ">$\n",
    "V_1(M) = \\max \\left[ 3 + 0.9(0.7V_0(M) + 0.3V_0(H)), 3 + 0.9(0.5V_0(M) + 0.5V_0(H)) \\right]\n",
    "$\n",
    "\n",
    "For **High Wealth (H):**\n",
    "\n",
    ">$\n",
    "V_1(H) = \\max \\left[ 5 + 0.9(0.9V_0(H) + 0.1V_0(M)), 5 + 0.9(0.7V_0(H) + 0.3V_0(M)) \\right]\n",
    "$\n",
    "\n",
    "Since $V_0(L) = V_0(M) = V_0(H) = 0$, the initial values are just the rewards.\n",
    "\n",
    ">$\n",
    "V_1(L) = -1, \\quad V_1(M) = 3, \\quad V_1(H) = 5\n",
    "$\n",
    "\n",
    "### By Bellman’s equation:\n",
    "\n",
    "$\n",
    "V_1(s) = \\max_a \\left[ R(s) + \\gamma \\sum_{s'} P(s' | s, a) V_0(s') \\right]\n",
    "$\n",
    "\n",
    "#### **Policy Evaluation after Iteration 1**\n",
    "\n",
    "\n",
    "\n",
    "#### **From state: L**\n",
    "- \\( Q(L, C) = -1 + 0.9(0.8(-1) + 0.2(3)) = -1.18 \\)\n",
    "- \\( Q(L, A) = -1 + 0.9(0.6(-1) + 0.4(3)) = -0.46 \\)\n",
    "- Q(L,A) > Q(L,C)\n",
    "- **Thus, the optimal action is A (Aggressive).**\n",
    "\n",
    "#### **From state: M**\n",
    "- \\( Q(M, C) = 3 + 0.9(0.7(3) + 0.3(5)) = 6.24 \\)\n",
    "- \\( Q(M, A) = 3 + 0.9(0.5(3) + 0.5(5)) = 6.6 \\)\n",
    "- Q(M,A) > Q(M,C)\n",
    "- **Therefore, taking action A (Aggressive) is the best choice.**\n",
    "\n",
    "#### **From state: H**\n",
    "- \\( Q(H, C) = 5 + 0.9(0.9(5) + 0.1(3)) = 9.32 \\)\n",
    "- \\( Q(H, A) = 5 + 0.9(0.7(5) + 0.3(3)) = 8.96 \\)\n",
    "- Q(H,C) > Q(H,A)\n",
    "- **So, H should follow policy C (Conservative).**\n",
    "\n",
    "### **Policy at Iteration 1**\n",
    "- **L** → **Aggressive (A)**\n",
    "- **M** → **Aggressive (A)**\n",
    "- **H** → **Conservative (C)**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### **Iteration 2**\n",
    "\n",
    "Updating $V_2(s)$:\n",
    "\n",
    ">$\n",
    "V_2(L) = \\max \\left[ -1 + 0.9(0.8(-1) + 0.2(3)), -1 + 0.9(0.6(-1) + 0.4(3)) \\right]\n",
    "$\n",
    "\n",
    ">$\n",
    "V_2(H) = \\max \\left[ 5 + 0.9(0.9(5) + 0.1(3)), 5 + 0.9(0.7(5) + 0.3(3)) \\right]\n",
    "$\n",
    "\n",
    ">$\n",
    "V_2(M) = \\max \\left[ 3 + 0.9(0.7(3) + 0.3(5)), 3 + 0.9(0.5(3) + 0.5(5)) \\right]\n",
    "$\n",
    "\n",
    "Computing the above:\n",
    "\n",
    "$V_2(L) = -0.46$\n",
    "\n",
    "$V_2(H) = 9.32$\n",
    "\n",
    "$V_2(M) = 6.6$\n",
    "\n",
    "\n",
    "#### **Policy Evaluation after Iteration 2**\n",
    "\n",
    "\n",
    "#### **State: L**\n",
    "- \\( Q(L, C) = -1 + 0.9(0.8(-0.46) + 0.2(6.6)) = -0.1432 \\)\n",
    "- \\( Q(L, A) = -1 + 0.9(0.6(-0.46) + 0.4(6.6)) = 1.1276 \\)\n",
    "- **Since \\( Q(L, A) > Q(L, C) \\), the best action is A (Aggressive).**\n",
    "\n",
    "#### **State: M**\n",
    "- \\( Q(M, C) = 3 + 0.9(0.7(6.6) + 0.3(9.32)) = 9.6744 \\)\n",
    "- \\( Q(M, A) = 3 + 0.9(0.5(6.6) + 0.5(9.32)) = 10.164 \\)\n",
    "- **Since \\( Q(M, A) > Q(M, C) \\), the best action is A (Aggressive).**\n",
    "\n",
    "#### **State: H**\n",
    "- \\( Q(H, C) = 5 + 0.9(0.9(9.32) + 0.1(6.6)) = 13.1432 \\)\n",
    "- \\( Q(H, A) = 5 + 0.9(0.7(9.32) + 0.3(6.6)) = 12.6536 \\)\n",
    "- **Since \\( Q(H, C) > Q(H, A) \\), the best action is C (Conservative).**\n",
    "\n",
    "**Policy at Iteration 2:**\n",
    "- **L** → **Aggressive (A)**\n",
    "- **M** → **Aggressive (A)**\n",
    "- **H** → **Conservative (C)**\n",
    "\n",
    "#### **Iteration 3**\n",
    "\n",
    "Updating $V_3(s)$:\n",
    "\n",
    "\n",
    ">$\n",
    "V_3(L) = \\max \\left[ -1 + 0.9(0.8(-0.46) + 0.2(6.6)), -1 + 0.9(0.6(-0.46) + 0.4(6.6)) \\right]\n",
    "$\n",
    "\n",
    ">$\n",
    "V_3(H) = \\max \\left[ 3 + 0.9(0.7(6.6) + 0.3(9.32)), 3 + 0.9(0.5(6.6) + 0.5(9.32)) \\right]\n",
    "$\n",
    "\n",
    ">$\n",
    "V_3(M) = \\max \\left[ 5 + 0.9(0.9(9.32) + 0.1(6.6)), 5 + 0.9(0.7(9.32) + 0.3(6.6)) \\right]\n",
    "$\n",
    "\n",
    "Computing the above:\n",
    "\n",
    "$V_3(L) = 1.1276$\n",
    "\n",
    "$V_3(H) = 13.1432$\n",
    "\n",
    "$V_3(M) = 10.164$\n",
    "\n",
    "\n",
    "#### **Policy Change Analysis**\n",
    "\n",
    "\n",
    "Evaluating the transition from **Iteration 2 to Iteration 3**, we analyze the action values to check for any policy updates.\n",
    "\n",
    "#### **State: L**\n",
    "- \\( Q(L, C) = -1 + 0.9(0.8(1.1276) + 0.2(10.164)) = 1.641392 \\)\n",
    "- \\( Q(L, A) = -1 + 0.9(0.6(1.1276) + 0.4(10.164)) = 3.267944 \\)\n",
    "- **Since \\( Q(L, A) > Q(L, C) \\), the best action remains A (Aggressive).**\n",
    "\n",
    "#### **State: M**\n",
    "- \\( Q(M, C) = 3 + 0.9(0.7(10.164) + 0.3(13.1432)) = 12.951984 \\)\n",
    "- \\( Q(M, A) = 3 + 0.9(0.5(10.164) + 0.5(13.1432)) = 13.48824 \\)\n",
    "- **Since \\( Q(M, A) > Q(M, C) \\), the best action remains A (Aggressive).**\n",
    "\n",
    "#### **State: H**\n",
    "- \\( Q(H, C) = 5 + 0.9(0.9(13.1432) + 0.1(10.164)) = 16.560752 \\)\n",
    "- \\( Q(H, A) = 5 + 0.9(0.7(13.1432) + 0.3(10.164)) = 16.024496 \\)\n",
    "- **Since \\( Q(H, C) > Q(H, A) \\), the best action remains C (Conservative).**\n",
    "\n",
    "### **Updated Policy at Iteration 3**\n",
    "After comparing \\( Q(L, A) \\) vs. \\( Q(L, C) \\) and \\( Q(H, C) \\) vs. \\( Q(H, A) \\), the policy remains unchanged:\n",
    "\n",
    "- **L (Low Wealth)** → **Aggressive (A)**\n",
    "- **M (Medium Wealth)** → **Aggressive (A)**\n",
    "- **H (High Wealth)** → **Conservative (C)**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0e5695-a01b-4bcc-909b-582b5e64d068",
   "metadata": {},
   "source": [
    "\n",
    "### Summary: Policy Evolution Over Iterations\n",
    "\n",
    "\n",
    "The table below illustrates the progression of action values across iterations for each state:\n",
    "\n",
    "| **State**  | **Iteration 1** | **Iteration 2** | **Iteration 3** |\n",
    "|------------|----------------|----------------|----------------|\n",
    "| **Low**    | -1.00          | -0.46         | 3.2679        |\n",
    "| **Medium** | 3.00           | 6.60          | 13.4882       |\n",
    "| **High**   | 5.00           | 9.32          | 16.5608       |\n",
    "\n",
    "### **Analysis & Observations**\n",
    "The results indicate a steady increase in the agent's expected rewards over successive iterations. Notably:  \n",
    "\n",
    "- An agent starting in the **low** state sees an improvement from an initial reward of **-1** to **3.27** by iteration 3, provided they consistently select the optimal action. This suggests that, under the current model, the agent has the potential to increase their wealth indefinitely with continued iterations.  \n",
    "- Similarly, agents in the **medium** and **high** wealth states experience continuous growth in expected rewards. Based on the update equations, these values will keep increasing if the agents persistently make the best decisions.  \n",
    "- A key limitation of this model is its deterministic nature—outcomes are entirely predictable given the agent’s choices. Introducing an element of **randomness** could enhance realism, reflecting the inherent uncertainty and external factors influencing wealth accumulation in real-world scenarios.  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
